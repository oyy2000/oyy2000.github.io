---
---

@article{haha,
  abbr={Submitted to ICML},
  bibtex_show={true},
  title={DenseSteer: Steering Small Language Models towards Dense Math Reasoning},
  author={Ouyang, Yang, and Lin, Shuhang, and Kim, Jung-Eun},
  journal={The 43rd International Conference on Machine Learnings},
  year={2026},
  selected={true}
}

@inproceedings{ouyang-etal-2025-layer,
    abbr={NAACL},
    bibtex_show={true},
    title = "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense",
    author = "Ouyang, Yang  and
      Gu, Hengrui  and
      Lin, Shuhang  and
      Hua, Wenyue  and
      Peng, Jie  and
      Kailkhura, Bhavya  and
      Gao, Meijun  and
      Chen, Tianlong  and
      Zhou, Kaixiong",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.623/",
    doi = "10.18653/v1/2025.naacl-long.623",
    pages = "12541--12554",
    ISBN = "979-8-89176-189-6",
    abstract = "As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to patch specific layers within LLMs through self-augmented datasets. Our insight is that certain layer(s), tend to produce affirmative tokens when faced with harmful prompts. By identifying these layers and adversarially exposing them to generate more harmful data, one can understand their inherent and diverse vulnerabilities to attacks. With these exposures, we then ``unlearn'' these issues, reducing the impact of affirmative tokens and hence minimizing jailbreak risks while keeping the model{'}s responses to safe queries intact.We conduct extensive experiments on two models, four benchmark datasets, and multiple state-of-the-art jailbreak attacks to demonstrate the efficacy of our approach. Results indicate that our framework reduces the harmfulness and attack success rate of jailbreak attacks without compromising utility for benign queries compared to recent defense methods. Our code is publicly available at: https://github.com/oyy2000/LayerAdvPatcher",
    pdf={https://aclanthology.org/2025.naacl-long.623.pdf},
    website={https://github.com/oyy2000/LayerAdvPatcher},
    selected={true}
}



@article{zhang2024min,
abbr={ICLR},
bibtex_show={true},
  title={Min-k\%++: Improved baseline for detecting pre-training data from large language models},
  author={Zhang*, Jingyang and Sun*, Jingwei and Yeats, Eric and Ouyang, Yang and Kuo, Martin and Zhang, Jianyi and Yang, Hao Frank and Li, Hai},
  journal={The Thirteenth International Conference on Learning Representations},
  award={Spotlight},
  year={2025},
  pdf={https://arxiv.org/pdf/2404.02936},
  website={https://zjysteven.github.io/mink-plus-plus/},
  selected={true}
}